{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BygAURlGNh0"
      },
      "source": [
        "# Importing data from Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB0tSBljfKzy"
      },
      "source": [
        "##Train and validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrX9f2RJ3U4o",
        "outputId": "74efcbe8-9871-4bb1-d29a-3384dc637bbb"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle\n",
        "!pip install -q kaggle-cli\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp \"/content/drive/MyDrive/kaggle.json\" ~/.kaggle/\n",
        "!cat ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d vaishnaviasonawane/indian-sign-language-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vV4iiY6C3Vp_"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "local_zip = '/content/indian-sign-language-dataset.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqt_djuJfOmC"
      },
      "source": [
        "##Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eN_a8pw9XRaY",
        "outputId": "3fb94bd3-c7e4-4845-bdf0-3aeeda53d2c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading indian-sign-language-isl.zip to /content\n",
            " 99% 278M/281M [00:10<00:00, 35.9MB/s]\n",
            "100% 281M/281M [00:10<00:00, 28.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d prathumarikeri/indian-sign-language-isl\n",
        "\n",
        "import zipfile\n",
        "local_zip = '/content/indian-sign-language-isl.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR7mKI37GYnF"
      },
      "source": [
        "#Splitting data into train and validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2HkmbAl3Vw4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import shutil\n",
        "\n",
        "rootdir= 'data'\n",
        "\n",
        "classes = ['1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
        "\n",
        "for i in classes:\n",
        "  os.makedirs(rootdir +'/train/' + i)\n",
        "  os.makedirs(rootdir +'/valid/' + i)\n",
        "  source = rootdir + '/' + i\n",
        "\n",
        "  allFileNames = os.listdir(source)\n",
        "  np.random.shuffle(allFileNames)\n",
        "  val_ratio = 0.3\n",
        "  train_FileNames, val_FileNames = np.split(np.array(allFileNames), [int(len(allFileNames)* (1 - val_ratio))])\n",
        "\n",
        "  train_FileNames = [source+'/'+ name for name in train_FileNames.tolist()]\n",
        "  val_FileNames = [source+'/' + name for name in val_FileNames.tolist()]\n",
        "\n",
        "  for name in train_FileNames:\n",
        "    shutil.copy(name, rootdir +'/train/' + i)\n",
        "\n",
        "  for name in val_FileNames:\n",
        "    shutil.copy(name, rootdir +'/valid/' + i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7atr_FFJaSU"
      },
      "source": [
        "#Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p7xeKzrNfSY"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eH479ketxc6"
      },
      "outputs": [],
      "source": [
        "train_dir = \"data/train\"\n",
        "validation_dir = \"data/valid\"\n",
        "test_dir = \"Indian\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oTOKhV_O38C"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.resnet_v2 import ResNet50V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LpJ6utsQjP5"
      },
      "outputs": [],
      "source": [
        "models = [InceptionV3, ResNet50V2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niYu3w9eJWSG"
      },
      "source": [
        "## InceptionV3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uhiSrC1dWzF",
        "outputId": "37064385-5976-4206-e093-1e271c78cb55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 29400 images belonging to 35 classes.\n",
            "Found 12600 images belonging to 35 classes.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " inception_v3 (Functional)   (None, 8, 8, 2048)        21802784  \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 131072)            0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 35)                4587555   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 26,390,339\n",
            "Trainable params: 4,587,555\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "230/230 [==============================] - 166s 702ms/step - loss: 0.2519 - acc: 0.9907 - precision_2: 0.9949 - recall_2: 0.9907 - auc_2: 0.9975 - val_loss: 1.9289e-08 - val_acc: 1.0000 - val_precision_2: 1.0000 - val_recall_2: 1.0000 - val_auc_2: 1.0000\n",
            "Epoch 2/2\n",
            "230/230 [==============================] - 163s 710ms/step - loss: 1.0542e-10 - acc: 1.0000 - precision_2: 1.0000 - recall_2: 1.0000 - auc_2: 1.0000 - val_loss: 1.8825e-08 - val_acc: 1.0000 - val_precision_2: 1.0000 - val_recall_2: 1.0000 - val_auc_2: 1.0000\n"
          ]
        }
      ],
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "train_dir,\n",
        "target_size=(299, 299),\n",
        "batch_size=128,\n",
        "class_mode='categorical')\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "validation_dir,\n",
        "target_size=(299, 299),\n",
        "batch_size=32,\n",
        "class_mode='categorical')\n",
        "\n",
        "base_model = models[0](input_shape=(299,299,3), weights='imagenet', include_top=False)\n",
        "base_model.trainable = False\n",
        "\n",
        "inc = tf.keras.Sequential([\n",
        "  base_model,\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(35, activation='softmax')\n",
        "])\n",
        "inc.summary()\n",
        "\n",
        "inc.compile(loss = 'categorical_crossentropy', metrics = [['acc', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()]])\n",
        "inc1 = inc.fit(train_generator, epochs=2, validation_data=validation_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XA1kdOroVQdo",
        "outputId": "b61a2143-d61e-42dd-c730-2ddee156a79c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 42745 images belonging to 35 classes.\n",
            "334/334 [==============================] - 165s 492ms/step - loss: 0.7354 - acc: 0.9875 - precision_2: 0.9875 - recall_2: 0.9875 - auc_2: 0.9937\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.7354331612586975,\n",
              " 0.9875307083129883,\n",
              " 0.9875307083129883,\n",
              " 0.9875307083129883,\n",
              " 0.9936813116073608]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_generator = test_datagen.flow_from_directory(\n",
        "test_dir,\n",
        "target_size=(299, 299),\n",
        "batch_size=128,\n",
        "class_mode='categorical')\n",
        "\n",
        "inc.evaluate(test_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlGNCy_BJcKW"
      },
      "source": [
        "##ResNet50V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-7kWr5tX_pg",
        "outputId": "28eaf181-7451-495d-cc95-676c3388c5fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 29400 images belonging to 35 classes.\n",
            "Found 12600 images belonging to 35 classes.\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet50v2 (Functional)     (None, 7, 7, 2048)        23564800  \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 100352)            0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 35)                3512355   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 27,077,155\n",
            "Trainable params: 3,512,355\n",
            "Non-trainable params: 23,564,800\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "230/230 [==============================] - 121s 514ms/step - loss: 0.1013 - acc: 0.9938 - precision_3: 0.9967 - recall_3: 0.9937 - auc_3: 0.9978 - val_loss: 7.6634e-10 - val_acc: 1.0000 - val_precision_3: 1.0000 - val_recall_3: 1.0000 - val_auc_3: 1.0000\n",
            "Epoch 2/2\n",
            "230/230 [==============================] - 117s 507ms/step - loss: 2.8383e-11 - acc: 1.0000 - precision_3: 1.0000 - recall_3: 1.0000 - auc_3: 1.0000 - val_loss: 1.4192e-10 - val_acc: 1.0000 - val_precision_3: 1.0000 - val_recall_3: 1.0000 - val_auc_3: 1.0000\n"
          ]
        }
      ],
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "train_dir,\n",
        "target_size=(224, 224),\n",
        "batch_size=128,\n",
        "class_mode='categorical')\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "validation_dir,\n",
        "target_size=(224, 224),\n",
        "batch_size=32,\n",
        "class_mode='categorical')\n",
        "\n",
        "base_model = models[1](input_shape=(224,224,3), weights='imagenet', include_top=False)\n",
        "base_model.trainable = False\n",
        "\n",
        "res = tf.keras.Sequential([\n",
        "  base_model,\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(35, activation='softmax')\n",
        "])\n",
        "res.summary()\n",
        "\n",
        "res.compile(loss = 'categorical_crossentropy', metrics = [['acc', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()]])\n",
        "res1 = res.fit(train_generator, epochs=2, validation_data=validation_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brFvtKzbWcLR",
        "outputId": "bc817b0c-4381-45bd-a490-088b094590e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 42745 images belonging to 35 classes.\n",
            "334/334 [==============================] - 119s 355ms/step - loss: 0.2589 - acc: 0.9868 - precision_3: 0.9875 - recall_3: 0.9868 - auc_3: 0.9941\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.2588743567466736,\n",
              " 0.9868288636207581,\n",
              " 0.9874750971794128,\n",
              " 0.9867820739746094,\n",
              " 0.9941183924674988]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_generator = test_datagen.flow_from_directory(\n",
        "test_dir,\n",
        "target_size=(224, 224),\n",
        "batch_size=128,\n",
        "class_mode='categorical')\n",
        "\n",
        "res.evaluate(test_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm6i__SKJg1T"
      },
      "source": [
        "#Saving the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ha_GVS1DfL79"
      },
      "outputs": [],
      "source": [
        "inc.save(\"/content/drive/MyDrive/DL_MP/inc/inc.h5\")\n",
        "res.save(\"/content/drive/MyDrive/DL_MP/res/res.h5\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
